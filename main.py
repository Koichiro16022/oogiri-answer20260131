import re
import os
import random
import asyncio
import numpy as np
import streamlit as st
import google.generativeai as genai
from PIL import Image, ImageDraw, ImageFont
from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip, AudioFileClip, CompositeAudioClip, concatenate_audioclips, AudioClip
from gtts import gTTS
import edge_tts

# --- 1. åŸºæœ¬è¨­å®š ---
if "GEMINI_API_KEY" in st.secrets:
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
else:
    st.error("APIã‚­ãƒ¼ãŒSecretsã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

CHOSEN_MODEL = 'models/gemini-2.0-flash'
FONT_PATH = "NotoSansJP-Bold.ttf"
BASE_VIDEO = "template.mp4"
SOUND1 = "sound1.mp3"
SOUND2 = "sound2.mp3"

st.set_page_config(page_title="å¤§å–œåˆ©ã‚¢ãƒ³ã‚µãƒ¼", layout="wide")

# UIãƒ‡ã‚¶ã‚¤ãƒ³ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
st.markdown("""
    <style>
    .main { background-color: #001220; color: #E5E5E5; }
    .stButton>button { width: 100%; border-radius: 5px; font-weight: bold; }
    div.stButton > button:first-child { background: linear-gradient(135deg, #FFD700 0%, #E5E5E5 100%); color: #001220; }
    .stVideo { max-width: 100%; margin: auto; }
    
    /* æ³¨é‡ˆãƒ†ã‚­ã‚¹ãƒˆï¼ˆé»’ï¼‰ */
    .pronounce-box { font-size: 0.8rem; color: black !important; margin-top: -10px; margin-bottom: 10px; }
    .odai-pronounce { font-size: 0.85rem; color: black !important; margin-top: -15px; margin-bottom: 10px; }
    
    /* å…¥åŠ›æ¬„ã®ãƒ©ãƒ™ãƒ«ï¼ˆèª¬æ˜æ–‡ï¼‰ã‚’ç™½ãå¤ªãã—ã¦è¦‹ã‚„ã™ãã™ã‚‹ */
    .stTextInput label, .stTextArea label {
        color: #FFFFFF !important;
        font-size: 1rem !important;
        font-weight: 800 !important;
        text-shadow: 1px 1px 2px #000000;
        margin-bottom: 5px;
    }

    /* å…¥åŠ›æ¬„ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢å«ã‚€ï¼‰ã®èƒŒæ™¯è‰²ã‚’æ·¡ã„æ°´è‰²ã«ã€æ–‡å­—ã‚’æ¿ƒã„é’ã« */
    div[data-baseweb="input"] > div, div[data-baseweb="base-input"] > textarea {
        background-color: #E1F5FE !important;
        color: #01579B !important;
        border-radius: 4px;
    }
    </style>
    """, unsafe_allow_html=True)

# --- 2. çŠ¶æ…‹ç®¡ç† ---
if 'kw' not in st.session_state: st.session_state.kw = "SNS"
if 'odais' not in st.session_state: st.session_state.odais = []
if 'selected_odai' not in st.session_state: st.session_state.selected_odai = ""
if 'selected_odai_pron' not in st.session_state: st.session_state.selected_odai_pron = ""
if 'ans_list' not in st.session_state: st.session_state.ans_list = []
if 'pronounce_list' not in st.session_state: st.session_state.pronounce_list = []

if 'golden_examples' not in st.session_state:
    st.session_state.golden_examples = [
        {"odai": "ç›®ã«å…¥ã‚Œã¦ã‚‚ç—›ããªã„å­«ã«ãŠã˜ã„ã¡ã‚ƒã‚“ãŒãƒ–ãƒã‚®ãƒ¬ã€‚ã„ã£ãŸã„ä½•ãŒã‚ã£ãŸï¼Ÿ", "ans": "ãŠã˜ã„ã¡ã‚ƒã‚“ã®å…¥ã‚Œæ­¯ã‚’ãƒ¡ãƒ«ã‚«ãƒªã§ã€ãƒ“ãƒ³ãƒ†ãƒ¼ã‚¸é›‘è²¨ã€ã¨ã—ã¦å‡ºå“ã—ã¦ã„ãŸ"},
        {"odai": "ã“ã®ç•ªçµ„çµ¶å¯¾ãƒ‰ãƒƒã‚­ãƒªã ã‚ï¼ãªãœæ°—ä»˜ã„ãŸï¼Ÿ", "ans": "é€šè¡Œäºº10äººå…¨å“¡ãŒã‚ˆãè¦‹ãŸã‚‰ã‚¨ã‚­ã‚¹ãƒˆãƒ©ã®ãƒã‚¤ãƒˆå‹Ÿé›†ã§è¦‹ã‹ã‘ãŸé¡”ã ã£ãŸ"},
        {"odai": "ãƒã‚²ã¦ã¦è‰¯ã‹ã£ãŸï½ãªãœãã†æ€ã£ãŸï¼Ÿ", "ans": "è·è³ªã®ãƒ—ãƒ­ã«ã€å›ã€éš ã—äº‹ãªã•ãã†ãªé ­ã—ã¦ã‚‹ã­ã€ã¨ã‚¹ãƒ«ãƒ¼ã•ã‚ŒãŸ"},
        {"odai": "ãƒã‚²ã¦ã¦è‰¯ã‹ã£ãŸï½ãªãœãã†æ€ã£ãŸï¼Ÿ", "ans": "ç¾å®¹å¸«ã•ã‚“ã«ãŠä»»ã›ã§ã¨è¨€ã£ãŸã‚‰3ç§’ã§ä¼šè¨ˆãŒçµ‚ã‚ã£ãŸ"},
        {"odai": "æ¯è¦ªãŒç§ã®å‹é”ã«å¤§æ¿€æ€’ã€‚ã„ã£ãŸã„ä½•ãŒã‚ã£ãŸï¼Ÿ", "ans": "å®¶æ—å†™çœŸã®ãŠæ¯ã•ã‚“ã®é¡”ã®éƒ¨åˆ†ã ã‘ã«åŸ·æ‹—ã«ã€ãƒ–ã‚µã‚¤ã‚¯ã«ãªã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€ã‚’ã‹ã‘ã¦ä¿å­˜ã—ãŸ"},
        {"odai": "ãŠã°ã•ã‚“ãã®æœã‚«ãƒ¼ãƒ†ãƒ³ã¨åŒã˜æŸ„ã§ã™ã­ï¼ã¨æ˜ã‚‹ãæŒ‡æ‘˜ã—ãŸ", "ans": "æ¯è¦ªãŒç§ã®å‹é”ã«å¤§æ¿€æ€’ã€‚ã„ã£ãŸã„ä½•ãŒã‚ã£ãŸï¼Ÿ"}
    ]

# --- 3. ãƒ­ã‚¸ãƒƒã‚¯ ---
async def save_edge_voice(text, filename, voice_name, rate="+15%"):
    communicate = edge_tts.Communicate(text, voice_name, rate=rate)
    await communicate.save(filename)

def make_silence(duration):
    return AudioClip(lambda t: [0, 0], duration=duration, fps=44100)

def build_controlled_audio(full_text, mode="gtts"):
    parts = re.split(r'(_+)', full_text)
    clips = []
    for i, part in enumerate(parts):
        if not part: continue
        if '_' in part:
            duration = len(part) * 0.1
            clips.append(make_silence(duration))
        else:
            tmp_filename = f"part_{mode}_{i}.mp3"
            if mode == "gtts":
                tts = gTTS(part, lang='ja')
                tts.save(tmp_filename)
            else:
                asyncio.run(save_edge_voice(part, tmp_filename, "ja-JP-KeitaNeural", rate="+15%"))
            clips.append(AudioFileClip(tmp_filename))
    if not clips: return None
    return concatenate_audioclips(clips)

def create_text_image(text, fontsize, color, pos=(960, 540)):
    img = Image.new("RGBA", (1920, 1080), (255, 255, 255, 0))
    draw = ImageDraw.Draw(img)
    try: font = ImageFont.truetype(FONT_PATH, fontsize)
    except: font = ImageFont.load_default()
    clean_display = text.replace("_", "")
    display_text = clean_display.replace("ã€€", "\n").replace(" ", "\n")
    lines = [l for l in display_text.split("\n") if l.strip()]
    if not lines: lines = [" "]
    line_spacing = 15
    line_heights = [draw.textbbox((0, 0), line, font=font)[3] - draw.textbbox((0, 0), line, font=font)[1] for line in lines]
    total_height = sum(line_heights) + (len(lines) - 1) * line_spacing
    current_y = pos[1] - total_height // 2
    for i, line in enumerate(lines):
        bbox = draw.textbbox((0, 0), line, font=font)
        line_w = bbox[2] - bbox[0]
        draw.text((pos[0] - line_w // 2, current_y), line, font=font, fill=color)
        current_y += line_heights[i] + line_spacing
    return np.array(img)

def create_geki_video(odai_display, odai_audio, answer_display, answer_audio):
    for f in [BASE_VIDEO, SOUND1, SOUND2]:
        if not os.path.exists(f): return None
    try:
        video = VideoFileClip(BASE_VIDEO).without_audio()
        clean_ans_disp = re.sub(r'^[\d\.ï¼ã€ã€‚\s\*]+', '', answer_display).strip()
        clean_ans_aud = re.sub(r'^[\d\.ï¼ã€ã€‚\s\*]+', '', answer_audio).strip()
        i1 = create_text_image(odai_display, 100, "black", pos=(960, 530)) 
        i2 = create_text_image(odai_display, 55, "black", pos=(880, 300))
        i3 = create_text_image(clean_ans_disp, 120, "black", pos=(960, 500))
        c1 = ImageClip(i1).set_start(2.0).set_end(8.0)
        c2 = ImageClip(i2).set_start(8.0).set_end(10.0)
        c3 = ImageClip(i3).set_start(10.0).set_end(16.0)
        voice_odai_clip = build_controlled_audio(odai_audio, mode="gtts")
        voice_ans_clip = build_controlled_audio(clean_ans_aud, mode="edge")
        audio_list = []
        if voice_odai_clip: audio_list.append(voice_odai_clip.set_start(2.5))
        if voice_ans_clip: audio_list.append(voice_ans_clip.set_start(10.5))
        audio_list.append(AudioFileClip(SOUND1).set_start(0.8).volumex(0.2))
        audio_list.append(AudioFileClip(SOUND2).set_start(9.0).volumex(0.3))
        final = CompositeVideoClip([video, c1, c2, c3], size=(1920, 1080)).set_audio(CompositeAudioClip(audio_list))
        out = "geki.mp4"
        final.write_videofile(out, fps=24, codec="libx264", audio_codec="aac", temp_audiofile='temp-audio.m4a', remove_temp=True, logger=None)
        video.close(); final.close()
        return out
    except Exception as e:
        st.error(f"åˆæˆå¤±æ•—: {e}"); return None

# --- 4. ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.header("ğŸ§  æ„Ÿæ€§åŒæœŸãƒ»è¿½åŠ å­¦ç¿’")
    with st.form("learning_form", clear_on_submit=True):
        new_odai = st.text_area("ãŠé¡Œã‚’è¿½åŠ ", height=100)
        new_ans = st.text_input("å›ç­”ã‚’è¿½åŠ ")
        if st.form_submit_button("æ„Ÿæ€§ã‚’è¦šãˆã•ã›ã‚‹"):
            if new_odai and new_ans:
                st.session_state.golden_examples.append({"odai": new_odai, "ans": new_ans})
                st.success("ç™»éŒ²ã—ã¾ã—ãŸã€‚")

# --- 5. ãƒ¡ã‚¤ãƒ³UI ---
st.title("å¤§å–œåˆ©ã‚¢ãƒ³ã‚µãƒ¼")
kw_col, clr_col, rnd_col = st.columns([5, 1, 1])
st.session_state.kw = kw_col.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›", value=st.session_state.kw, label_visibility="collapsed")
if clr_col.button("æ¶ˆå»"): st.session_state.kw = ""; st.rerun()
if rnd_col.button("ãƒ©ãƒ³ãƒ€ãƒ "): st.session_state.kw = random.choice(["SNS", "å¤ç•‘ä»»ä¸‰éƒ", "æ¯è¦ª", "ã‚µã‚¦ãƒŠ"]); st.rerun()

if st.button("ãŠé¡Œç”Ÿæˆ", use_container_width=True):
    with st.spinner("å³é¸ä¸­..."):
        m = genai.GenerativeModel(CHOSEN_MODEL)
        prompt = f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{st.session_state.kw}ã€ã‚’ä½¿ã£ãŸå¤§å–œåˆ©ãŠé¡Œã‚’3ã¤ã€‚èª¬æ˜ä¸è¦ã€‚3è¡Œã§ã€‚"
        r = m.generate_content(prompt)
        st.session_state.odais = [re.sub(r'^[\d\.ï¼\s]+', '', l).strip() for l in r.text.split('\n') if l.strip()][:3]
        st.session_state.selected_odai = ""; st.session_state.ans_list = []; st.rerun()

if st.session_state.odais:
    for i, o in enumerate(st.session_state.odais):
        if st.button(o, key=f"o_{i}"): 
            st.session_state.selected_odai = o
            st.session_state.selected_odai_pron = o
            st.session_state.ans_list = []; st.rerun()

if st.session_state.selected_odai:
    st.write("---")
    # ãŠé¡Œç¢ºå®šã®å…¥åŠ›æ¬„
    st.session_state.selected_odai = st.text_input("ãŠé¡Œç¢ºå®šï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã§æ”¹è¡Œï¼‰", value=st.session_state.selected_odai)
    # ãŠé¡Œèª­ã¿ä¿®æ­£ã®å…¥åŠ›æ¬„
    st.session_state.selected_odai_pron = st.text_input("ãŠé¡Œã®èª­ã¿ä¿®æ­£ï¼ˆ _ ã§ç„¡éŸ³ã®ã‚¿ãƒ¡ï¼‰", value=st.session_state.selected_odai_pron)
    st.markdown(f'<p class="odai-pronounce">â†‘ ãŠé¡Œã®ç™ºéŸ³ä¿®æ­£</p>', unsafe_allow_html=True)
    
    style = st.selectbox("ãƒ¦ãƒ¼ãƒ¢ã‚¢", ["é€šå¸¸", "çŸ¥çš„", "ã‚·ãƒ¥ãƒ¼ãƒ«", "ãƒ–ãƒ©ãƒƒã‚¯"])
    if st.button("å›ç­”20æ¡ˆç”Ÿæˆ", type="primary"):
        with st.spinner("çˆ†ç¬‘ã‚’è¿½æ±‚ä¸­..."):
            m = genai.GenerativeModel(CHOSEN_MODEL)
            ex_str = "\n".join([f"ãƒ»{e['ans']}" for e in st.session_state.golden_examples])
            p = f"""ã‚ãªãŸã¯ä¼èª¬ã®å¤§å–œåˆ©èŠ¸äººã§ã™ã€‚æŒ¨æ‹¶ãƒ»å‰ç½®ãå³ç¦ã€‚
            ãŠé¡Œ: {st.session_state.selected_odai}
            æ‰‹æœ¬: {ex_str}
            æŒ‡ç¤º: 20å€‹ã®å›ç­”ã‚’ã€Œ1. å›ç­”ã€å½¢å¼ã§ã€‚"""
            r = m.generate_content(p)
            ans_raw = [l.strip() for l in r.text.split('\n') if re.match(r'^\d+[\.ï¼ã€ã€‚\s]', l)][:20]
            st.session_state.ans_list = ans_raw
            st.session_state.pronounce_list = ans_raw[:]
            st.rerun()

if st.session_state.ans_list:
    st.write("### å›ç­”ä¸€è¦§")
    for i in range(len(st.session_state.ans_list)):
        col_t, col_g = st.columns([9, 1])
        st.session_state.ans_list[i] = col_t.text_input(f"å­—å¹•æ¡ˆ {i+1}ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã§æ”¹è¡Œï¼‰", value=st.session_state.ans_list[i], key=f"disp_{i}")
        st.session_state.pronounce_list[i] = st.text_input(f"èª­ã¿æ¡ˆ {i+1}ï¼ˆ _ ã§ç„¡éŸ³ã®ã‚¿ãƒ¡ï¼‰", value=st.session_state.pronounce_list[i], key=f"pron_{i}", label_visibility="collapsed")
        st.markdown(f'<p class="pronounce-box">â†‘ èª­ã¿ä¿®æ­£</p>', unsafe_allow_html=True)
        if col_g.button("ç”Ÿæˆ", key=f"b_{i}"):
            with st.spinner("å‹•ç”»ç”Ÿæˆä¸­..."):
                path = create_geki_video(st.session_state.selected_odai, st.session_state.selected_odai_pron, st.session_state.ans_list[i], st.session_state.pronounce_list[i])
                if path:
                    st.video(path)
                    with open(path, "rb") as f:
                        st.download_button("ä¿å­˜", f, file_name=f"geki_{i}.mp4", key=f"dl_{i}")

st.write("---")
st.caption("ã€Œç§ãŒ100%åˆ¶å¾¡ã—ã¦ã„ã¾ã™ã€")
